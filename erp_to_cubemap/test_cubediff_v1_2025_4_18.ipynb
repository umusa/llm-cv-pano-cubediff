{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeDiff Example Notebook\n",
    "\n",
    "This notebook demonstrates the CubeDiff architecture for generating high-quality 360Â° panoramas from text prompts and narrow field-of-view images.\n",
    "\n",
    "The notebook covers:\n",
    "1. Installation and setup (done in /Users/jinxuding/Downloads/CV/cubediff/implementation/llm-cv-pano-cubediff/test_erp_cubemap_conversion_v1_2025_4_18.ipynb)\n",
    "2. Testing the cubemap conversion functions (done in /Users/jinxuding/Downloads/CV/cubediff/implementation/llm-cv-pano-cubediff/test_erp_cubemap_conversion_v1_2025_4_18.ipynb)\n",
    "3. Loading model components\n",
    "4. Testing the synchronized GroupNorm and inflated attention\n",
    "5. Running inference with a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.24.0 transformers==4.36.2 torch==2.1.2 torchvision==0.16.2 accelerate==0.25.0 \\\n",
    "    opencv-python==4.8.1.78 matplotlib==3.8.2 tqdm==4.66.1 einops==0.7.0 huggingface_hub==0.19.4 opencv-python xformers requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Import your original utilities\n",
    "# from cubediff_utils_v2 import * \n",
    "from cubediff_utils_v1 import * \n",
    "from cubediff_utils_v2 import * \n",
    "from cubediff_utils import * \n",
    "\n",
    "import importlib\n",
    "import cubediff_utils as cu\n",
    "importlib.reload(cu)         # guarantees the notebook sees the new code\n",
    "import cubediff_utils_v2 as cu_v2\n",
    "importlib.reload(cu_v2)         # guarantees the notebook sees the new code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Model Components\n",
    "\n",
    "Let's load the pretrained model components from Stable Diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model functions\n",
    "from cubediff_models import load_sd_components, convert_attention_modules #, debug_convert_attention_modules # convert_to_inflated_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "vae, text_encoder, tokenizer, unet = load_sd_components(\n",
    "    model_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "    use_sync_gn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UNet to use inflated attention\n",
    "# unet = convert_to_inflated_attention(unet)\n",
    "# unet = convert_attention_modules(unet)\n",
    "# unet = debug_convert_attention_modules(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Synchronized GroupNorm and Inflated Attention\n",
    "\n",
    "Now let's test the synchronized GroupNorm and inflated attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the positional encoding\n",
    "# Create random latents\n",
    "batch_size = 1\n",
    "faces = 6\n",
    "channels = 4\n",
    "height = width = 64  # Latent space size\n",
    "\n",
    "latents = torch.randn(batch_size * faces, channels, height, width, device=device)\n",
    "\n",
    "# Add positional encodings\n",
    "latents_with_pos = add_cubemap_positional_encodings(latents, batch_size)\n",
    "\n",
    "# Check shape\n",
    "print(f\"Original latents shape: {latents.shape}\")\n",
    "print(f\"Latents with positional encodings shape: {latents_with_pos.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings for one batch\n",
    "pos_enc = latents_with_pos[0:6, channels:, :, :].cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "face_names = ['Front', 'Right', 'Back', 'Left', 'Top', 'Bottom']\n",
    "\n",
    "for i, name in enumerate(face_names):\n",
    "    # U coordinate\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.imshow(pos_enc[i, 0].numpy(), cmap='viridis')\n",
    "    plt.title(f\"{name} (U)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # V coordinate\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.imshow(pos_enc[i, 1].numpy(), cmap='viridis')\n",
    "    plt.title(f\"{name} (V)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Inference\n",
    "\n",
    "Now let's use the CubeDiff model to generate panoramas from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import inference class\n",
    "from cubediff_inference import CubeDiffInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    num_train_timesteps=1000,\n",
    "    clip_sample=False,\n",
    "    prediction_type=\"epsilon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "# Create inference pipeline\n",
    "pipeline = CubeDiffInference(\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device\n",
    ")\n",
    "# Then when loading the model, set the attention processor\n",
    "pipeline.unet.set_attn_processor(AttnProcessor2_0())\n",
    "# pipeline.enable_model_cpu_offload()  # This offloads to CPU when not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate panorama from text prompt\n",
    "prompt = \"A beautiful mountain landscape at sunset with a lake in the foreground\"\n",
    "torch.cuda.empty_cache()\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    result = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=80,  # Reduced for faster inference\n",
    "        guidance_scale=7.5,\n",
    "        seed=42,\n",
    "        return_faces=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Print result shape for debugging\n",
    "# print(\"Result type:\", type(result))\n",
    "# if isinstance(result, torch.Tensor):\n",
    "#     print(\"Result shape:\", result.shape)\n",
    "\n",
    "# # Visualize all aspects (individual faces, panorama, and 3D cube)\n",
    "# viz.visualize_all(result, prompt)\n",
    "\n",
    "\n",
    "# 2025-4-13 ------------------------------\n",
    "# Cell [16]: Generate cubemap faces from a text prompt using your CubeDiff model\n",
    "from cubediff_inference import generate_cubemap_from_prompt\n",
    "from generate_and_visualize import generate_and_visualize_cubemap\n",
    "\n",
    "# Generate and visualize cubemap faces\n",
    "prompt = \"A scenic mountain landscape with a lake and forest\"\n",
    "\n",
    "# If your model outputs faces in a different order than expected,\n",
    "# provide a correction mapping. For example:\n",
    "# face_order_correction = [3, 1, 0, 2, 4, 5] \n",
    "# Replace with None if no correction is needed\n",
    "face_order_correction = None\n",
    "\n",
    "# Run the generation and visualization\n",
    "results = generate_and_visualize_cubemap(\n",
    "    prompt=prompt,\n",
    "    model_inference_function=generate_cubemap_from_prompt,\n",
    "    face_size=512,\n",
    "    face_order_correction=face_order_correction\n",
    ")\n",
    "\n",
    "# Access the generated faces and panorama\n",
    "generated_faces = results['faces']\n",
    "generated_equirect = results['equirect']\n",
    "\n",
    "# Print quality metrics if available\n",
    "if 'metrics' in results and results['metrics']:\n",
    "    print(f\"MSE: {results['metrics'].get('mse', 'N/A')}\")\n",
    "    print(f\"PSNR: {results['metrics'].get('psnr', 'N/A')} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
