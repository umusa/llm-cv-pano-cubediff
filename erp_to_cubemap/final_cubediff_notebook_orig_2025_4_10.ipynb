{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeDiff Example Notebook\n",
    "\n",
    "This notebook demonstrates the CubeDiff architecture for generating high-quality 360Â° panoramas from text prompts and narrow field-of-view images.\n",
    "\n",
    "The notebook covers:\n",
    "1. Installation and setup\n",
    "2. Testing the cubemap conversion functions\n",
    "3. Loading model components\n",
    "4. Testing the synchronized GroupNorm and inflated attention\n",
    "5. Running inference with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the required packages and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.24.0 transformers==4.36.2 torch==2.1.2 torchvision==0.16.2 accelerate==0.25.0 \\\n",
    "    opencv-python==4.8.1.78 matplotlib==3.8.2 tqdm==4.66.1 einops==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, DDIMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print the number of available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing the Cubemap Conversion Functions\n",
    "\n",
    "Now let's implement and test the equirectangular to cubemap conversion functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cubemap utility functions\n",
    "from cubediff_utils import equirect_to_cubemap, cubemap_to_equirect, add_cubemap_positional_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample panorama image\n",
    "sample_url = \"https://cdn.polyhaven.com/asset_img/renders/vieux_port_marseille/360_equirectangular.png\"\n",
    "response = requests.get(sample_url)\n",
    "panorama_img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize for faster processing\n",
    "panorama_img = panorama_img.resize((2048, 1024))\n",
    "\n",
    "# Display sample panorama\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(panorama_img)\n",
    "plt.title(\"Sample Equirectangular Panorama\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to cubemap\n",
    "face_size = 512  # Size of each face\n",
    "cubemap_faces = equirect_to_cubemap(panorama_img, face_size=face_size)\n",
    "\n",
    "# Display cubemap faces\n",
    "face_names = ['Front', 'Right', 'Back', 'Left', 'Top', 'Bottom']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (face, name) in enumerate(zip(cubemap_faces, face_names)):\n",
    "    axes[i].imshow(face)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to equirectangular to validate the conversion\n",
    "equirect_reconstructed = cubemap_to_equirect(cubemap_faces, output_height=1024, output_width=2048)\n",
    "\n",
    "# Display reconstructed panorama\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(equirect_reconstructed.astype(np.uint8))\n",
    "plt.title(\"Reconstructed Equirectangular Panorama\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Model Components\n",
    "\n",
    "Let's load the pretrained model components from Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model functions\n",
    "from cubediff_models import load_sd_components, convert_to_inflated_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "vae, text_encoder, tokenizer, unet = load_sd_components(\n",
    "    model_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "    use_sync_gn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UNet to use inflated attention\n",
    "unet = convert_to_inflated_attention(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Synchronized GroupNorm and Inflated Attention\n",
    "\n",
    "Now let's test the synchronized GroupNorm and inflated attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the positional encoding\n",
    "# Create random latents\n",
    "batch_size = 1\n",
    "faces = 6\n",
    "channels = 4\n",
    "height = width = 64  # Latent space size\n",
    "\n",
    "latents = torch.randn(batch_size * faces, channels, height, width, device=device)\n",
    "\n",
    "# Add positional encodings\n",
    "latents_with_pos = add_cubemap_positional_encodings(latents, batch_size)\n",
    "\n",
    "# Check shape\n",
    "print(f\"Original latents shape: {latents.shape}\")\n",
    "print(f\"Latents with positional encodings shape: {latents_with_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings for one batch\n",
    "pos_enc = latents_with_pos[0:6, channels:, :, :].cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "face_names = ['Front', 'Right', 'Back', 'Left', 'Top', 'Bottom']\n",
    "\n",
    "for i, name in enumerate(face_names):\n",
    "    # U coordinate\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.imshow(pos_enc[i, 0].numpy(), cmap='viridis')\n",
    "    plt.title(f\"{name} (U)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # V coordinate\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.imshow(pos_enc[i, 1].numpy(), cmap='viridis')\n",
    "    plt.title(f\"{name} (V)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Inference\n",
    "\n",
    "Now let's use the CubeDiff model to generate panoramas from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import inference class\n",
    "from cubediff_inference import CubeDiffInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    num_train_timesteps=1000,\n",
    "    clip_sample=False,\n",
    "    prediction_type=\"epsilon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference pipeline\n",
    "pipeline = CubeDiffInference(\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate panorama from text prompt\n",
    "prompt = \"A beautiful mountain landscape at sunset with a lake in the foreground\"\n",
    "result = pipeline.generate(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=30,  # Reduced for faster inference\n",
    "    guidance_scale=7.5,\n",
    "    seed=42,\n",
    "    return_faces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated panorama\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(result[\"panorama\"])\n",
    "plt.title(f\"Generated Panorama: {prompt}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual cubemap faces\n",
    "face_names = ['Front', 'Right', 'Back', 'Left', 'Top', 'Bottom']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (face, name) in enumerate(zip(result[\"faces\"], face_names)):\n",
    "    axes[i].imshow(face)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image-to-Panorama Generation\n",
    "\n",
    "Let's test generating a panorama from a single input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image or use your own\n",
    "sample_img_url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4\"\n",
    "response = requests.get(sample_img_url)\n",
    "input_img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize for faster processing\n",
    "input_img = input_img.resize((512, 512))\n",
    "\n",
    "# Display sample input image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(input_img)\n",
    "plt.title(\"Input Image (Front Face)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate panorama from input image\n",
    "prompt = \"A majestic mountain range with a clear blue sky\"\n",
    "result_img2pano = pipeline.generate_from_image(\n",
    "    image=input_img,\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=30,  # Reduced for faster inference\n",
    "    guidance_scale=7.5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated panorama\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(result_img2pano[\"panorama\"])\n",
    "plt.title(f\"Generated Panorama from Input Image: {prompt}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving and Loading Models\n",
    "\n",
    "Let's demonstrate how to save and load trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(\"./output/checkpoints\", exist_ok=True)\n",
    "\n",
    "# Save UNet (pretending we've trained it)\n",
    "unet.save_pretrained(\"./output/checkpoints/unet_example\")\n",
    "print(\"Saved UNet to ./output/checkpoints/unet_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved UNet\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "loaded_unet = UNet2DConditionModel.from_pretrained(\"./output/checkpoints/unet_example\")\n",
    "print(\"Loaded UNet from checkpoint\")\n",
    "\n",
    "# Convert to inflated attention\n",
    "loaded_unet = convert_to_inflated_attention(loaded_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Optimization for L4 GPUs\n",
    "\n",
    "Let's explore memory optimization techniques for L4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimization functions\n",
    "from cubediff_optimization import optimize_for_vertex_ai, enable_memory_efficient_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimized settings for Vertex AI\n",
    "settings = optimize_for_vertex_ai(num_gpus=4, memory_per_gpu=22)\n",
    "print(\"Optimized settings for Vertex AI:\")\n",
    "for key, value in settings.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory-efficient attention\n",
    "if torch.cuda.is_available():\n",
    "    unet = enable_memory_efficient_attention(unet)\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    print(\"Enabled memory optimization techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring Memory Usage\n",
    "\n",
    "Let's create a function to monitor GPU memory usage during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory_stats():\n",
    "    \"\"\"Print GPU memory usage statistics\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available\")\n",
    "        return\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total_memory = torch.cuda.get_device_properties(i).total_memory / 1e9  # GB\n",
    "        reserved_memory = torch.cuda.memory_reserved(i) / 1e9  # GB\n",
    "        allocated_memory = torch.cuda.memory_allocated(i) / 1e9  # GB\n",
    "        free_memory = total_memory - reserved_memory  # GB\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  Reserved memory: {reserved_memory:.2f} GB\")\n",
    "        print(f\"  Allocated memory: {allocated_memory:.2f} GB\")\n",
    "        print(f\"  Free memory: {free_memory:.2f} GB\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor memory usage before and after model loading\n",
    "if torch.cuda.is_available():\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Initial memory usage:\")\n",
    "    print_gpu_memory_stats()\n",
    "    \n",
    "    # Load model components to device\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    unet = unet.to(device)\n",
    "    \n",
    "    print(\"\\nMemory usage after loading models:\")\n",
    "    print_gpu_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Benchmarking Inference Speed\n",
    "\n",
    "Let's benchmark the inference speed for different numbers of denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(pipeline, prompt, steps_list=[10, 20, 30, 50]):\n",
    "    \"\"\"Benchmark inference speed for different numbers of steps\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for steps in steps_list:\n",
    "        print(f\"Testing with {steps} steps...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Run once to warm up\n",
    "        _ = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=7.5,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Measure time for 3 runs\n",
    "        times = []\n",
    "        for i in range(3):\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _ = pipeline.generate(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=7.5,\n",
    "                seed=42+i\n",
    "            )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = sum(times) / len(times)\n",
    "        results[steps] = avg_time\n",
    "        print(f\"  Average time: {avg_time:.2f} seconds\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks (comment out if not needed, as this takes time)\n",
    "if torch.cuda.is_available():\n",
    "    prompt = \"A beautiful mountain landscape at sunset\"\n",
    "    benchmark_results = benchmark_inference(pipeline, prompt, steps_list=[10, 20, 30])\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    steps = list(benchmark_results.keys())\n",
    "    times = list(benchmark_results.values())\n",
    "    plt.bar(steps, times)\n",
    "    plt.xlabel('Number of Denoising Steps')\n",
    "    plt.ylabel('Inference Time (seconds)')\n",
    "    plt.title('Inference Time vs. Number of Denoising Steps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Creating a Sample Dataset\n",
    "\n",
    "Let's create a small example dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset creation function\n",
    "from cubediff_utils import create_example_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small example dataset\n",
    "create_example_dataset(num_samples=2, output_dir=\"./example_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Testing Dataset Loading\n",
    "\n",
    "Let's test the dataset loading and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset class\n",
    "from cubediff_dataset import CubemapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get panorama paths and prompts\n",
    "panorama_paths = [os.path.join(\"./example_data/panoramas\", f) for f in os.listdir(\"./example_data/panoramas\") \n",
    "                  if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "with open(\"./example_data/prompts.txt\", 'r') as f:\n",
    "    prompts = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Make sure we have a prompt for each panorama\n",
    "if len(prompts) < len(panorama_paths):\n",
    "    prompts = prompts * (len(panorama_paths) // len(prompts) + 1)\n",
    "prompts = prompts[:len(panorama_paths)]\n",
    "\n",
    "# Create transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CubemapDataset(\n",
    "    panorama_paths=panorama_paths,\n",
    "    text_prompts=prompts,\n",
    "    face_size=512,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from the dataset\n",
    "sample = dataset[0]\n",
    "cubemap_tensor = sample[\"cubemap\"]\n",
    "prompt = sample[\"prompt\"]\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Cubemap tensor shape: {cubemap_tensor.shape}\")\n",
    "\n",
    "# Convert tensor to image for visualization\n",
    "cubemap_images = [tensor.permute(1, 2, 0).numpy() for tensor in cubemap_tensor]\n",
    "\n",
    "# Display cubemap faces\n",
    "face_names = ['Front', 'Right', 'Back', 'Left', 'Top', 'Bottom']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (face, name) in enumerate(zip(cubemap_images, face_names)):\n",
    "    axes[i].imshow(face)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "This notebook has demonstrated the key components of the CubeDiff architecture:\n",
    "\n",
    "1. The conversion between equirectangular and cubemap representations\n",
    "2. The synchronized GroupNorm for color consistency across faces\n",
    "3. The inflated attention layers for cross-view awareness\n",
    "4. The inference pipeline for generating panoramas from text or images\n",
    "5. Memory optimization techniques for the Vertex AI environment\n",
    "\n",
    "The CubeDiff architecture represents a significant advancement in 360Â° panorama generation, providing high-quality, consistent results with minimal architectural modifications to existing diffusion models.\n",
    "\n",
    "To train your own CubeDiff model, use the provided `train_cubediff.py` script with a dataset of panoramic images. For inference with a pretrained model, use the `run_inference.py` script, which supports both text-to-panorama and image-to-panorama generation.\n",
    "\n",
    "For more details, refer to the documentation in the README file and the original paper \"CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
