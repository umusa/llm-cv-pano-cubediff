{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeDiff: Generating High-Quality 360° Panoramas\n",
    "\n",
    "This notebook demonstrates how to use the CubeDiff model to generate high-quality 360° panoramas from text prompts and normal field-of-view (NFoV) images.\n",
    "\n",
    "CubeDiff is based on the paper [\"CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation\"](https://arxiv.org/pdf/2501.17162)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers==0.24.0 transformers==4.36.2 torch==2.1.2 torchvision==0.16.2 accelerate==0.25.0 \\\n",
    "#     opencv-python==4.8.1.78 matplotlib==3.8.2 tqdm==4.66.1 einops==0.7.0 huggingface_hub==0.19.4 opencv-python xformers requests pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Import CubeDiff modules\n",
    "from data import (\n",
    "    CubemapDataset, \n",
    "    visualize_cubemap, \n",
    "    equirectangular_to_cubemap_batch, \n",
    "    cubemap_to_equirectangular_batch\n",
    ")\n",
    "from modules import (\n",
    "    CubemapPositionalEncoding,\n",
    "    GroupNormalizationSync,\n",
    "    InflatedAttention,\n",
    "    OverlappingEdgeProcessor,\n",
    "    adapt_unet_for_cubemap\n",
    ")\n",
    "from model import CubeDiff, CubeDiffPipeline\n",
    "from trainer import CubeDiffTrainer\n",
    "\n",
    "import importlib\n",
    "import model as md\n",
    "importlib.reload(md)         # guarantees the notebook sees the new code\n",
    "import modules as mds\n",
    "importlib.reload(mds)         # guarantees the notebook sees the new code\n",
    "import data as dt\n",
    "importlib.reload(dt)         # guarantees the notebook sees the new code\n",
    "import trainer as trn\n",
    "importlib.reload(trn)         # guarantees the notebook sees the new code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing the Cubemap Conversion\n",
    "\n",
    "Let's first test the equirectangular-to-cubemap conversion and vice versa using the provided utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image not found at sample_equirect.jpg. Please provide a valid path.\n",
      "CPU times: user 87 μs, sys: 0 ns, total: 87 μs\n",
      "Wall time: 77 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load a sample equirectangular image\n",
    "# Replace this with your own sample image path\n",
    "sample_path = \"sample_equirect.jpg\"\n",
    "\n",
    "if os.path.exists(sample_path):\n",
    "    # Load the image\n",
    "    equirect_img = np.array(Image.open(sample_path))\n",
    "    \n",
    "    # Display the equirectangular image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(equirect_img)\n",
    "    plt.title(\"Original Equirectangular Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert to cubemap\n",
    "    from cubediff_utils_v1 import improved_equirect_to_cubemap, optimized_cubemap_to_equirect\n",
    "    cube_faces = improved_equirect_to_cubemap(equirect_img, face_size=256)\n",
    "    \n",
    "    # Create a tensor from the cubemap\n",
    "    face_order = ['front', 'back', 'left', 'right', 'top', 'bottom']\n",
    "    cubemap_list = []\n",
    "    \n",
    "    for face_name in face_order:\n",
    "        face = cube_faces[face_name]\n",
    "        # Convert to tensor\n",
    "        face_tensor = torch.from_numpy(face).float().permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
    "        if face_tensor.max() > 1.0:\n",
    "            face_tensor = face_tensor / 255.0\n",
    "        cubemap_list.append(face_tensor)\n",
    "    \n",
    "    cubemap_tensor = torch.stack(cubemap_list, dim=0)  # (6, C, H, W)\n",
    "    \n",
    "    # Visualize the cubemap\n",
    "    visualize_cubemap(cubemap_tensor, title=\"Cubemap Faces\")\n",
    "    \n",
    "    # Convert back to equirectangular\n",
    "    equirect_recon = optimized_cubemap_to_equirect(cube_faces, *equirect_img.shape[:2])\n",
    "    \n",
    "    # Display the reconstructed equirectangular image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(equirect_recon)\n",
    "    plt.title(\"Reconstructed Equirectangular Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Sample image not found at {sample_path}. Please provide a valid path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the CubeDiff Model\n",
    "\n",
    "Now let's initialize the CubeDiff model using a pretrained Stable Diffusion model as the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CubeDiff with torch.float16 precision on cuda\n",
      "Loading pretrained model components...\n",
      "✓ Loaded tokenizer\n",
      "✓ Loaded text encoder (frozen)\n",
      "✓ Loaded VAE (frozen)\n",
      "✓ Loaded UNet\n",
      "✓ Loaded scheduler\n",
      "Adapting UNet for cubemap processing...\n",
      "Adapting attention layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: down_blocks.0.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: down_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: down_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.1.attentions.2.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.2.attentions.2.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.3.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.3.attentions.1.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "Adapting attention layer: up_blocks.3.attentions.2.transformer_blocks.0.attn1\n",
      "Adapting attention layer: up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "Adapting attention layer: mid_block.attentions.0.transformer_blocks.0.attn1\n",
      "Adapting attention layer: mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "Modified 32 attention layers to support cubemap inputs\n",
      "Adding cubemap-specific modules...\n",
      "Total parameters: 1,289,952,459\n",
      "Trainable parameters: 99,926,432 (7.75%)\n",
      "CubeDiff model initialization complete!\n",
      "CPU times: user 2.54 s, sys: 1.22 s, total: 3.76 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the CubeDiff model\n",
    "model = CubeDiff(\n",
    "    pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-base\",\n",
    "    use_fp16=True,  # Use mixed precision for faster inference\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Text-to-Panorama Generation\n",
    "\n",
    "Let's test generating a panorama from a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating panorama from prompt: A beautiful sunset over the ocean with mountains in the distance\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 4, 128]), adj_edge_region torch.Size([1, 4, 64, 4])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 4, 128]), adj_edge_region torch.Size([1, 4, 64, 4])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 4, 128]), adj_edge_region torch.Size([1, 4, 64, 4])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 4, 128]), adj_edge_region torch.Size([1, 4, 64, 4])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 64, 4]), adj_edge_region torch.Size([1, 4, 4, 128])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 64, 4]), adj_edge_region torch.Size([1, 4, 4, 128])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 64, 4]), adj_edge_region torch.Size([1, 4, 4, 128])\n",
      "Dimension mismatch: edge_region torch.Size([1, 4, 64, 4]), adj_edge_region torch.Size([1, 4, 4, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (49152) must match the size of tensor b (8192) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "File \u001b[0;32m~/mluser/git/llm-cv-pano-cubediff/architecture/model.py:767\u001b[0m, in \u001b[0;36mCubeDiffPipeline.__call__\u001b[0;34m(self, prompt, negative_prompt, input_image, condition_face, num_inference_steps, guidance_scale, generator, output_type, height, width)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;66;03m# Call the generate method of the model\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 767\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcondition_face\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_face\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mluser/git/llm-cv-pano-cubediff/architecture/model.py:555\u001b[0m, in \u001b[0;36mCubeDiff.generate\u001b[0;34m(self, prompt, negative_prompt, input_image, condition_face, num_inference_steps, guidance_scale, generator, output_type, height, width)\u001b[0m\n\u001b[1;32m    552\u001b[0m     condition_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_mask(batch_size, condition_face, num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Denoise latents with the diffusion model\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_latents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Decode the latents to images\u001b[39;00m\n\u001b[1;32m    565\u001b[0m cubemap_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_latents_to_image(latents)\n",
      "File \u001b[0;32m~/mluser/git/llm-cv-pano-cubediff/architecture/model.py:415\u001b[0m, in \u001b[0;36mCubeDiff.denoise_latents\u001b[0;34m(self, latents, text_embeddings, condition_mask, condition_latents, timesteps, guidance_scale)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Predict noise\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Forward pass through the UNet\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_times_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Reshape back to (B, num_faces, C, H, W)\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_faces, latent_channels, height, width\n\u001b[1;32m    424\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:1075\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:1160\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n\u001b[0;32m-> 1160\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/transformer_2d.py:392\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    380\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    381\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    382\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    390\u001b[0m         )\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/models/attention.py:329\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    321\u001b[0m         norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(norm_hidden_states)\n\u001b[1;32m    323\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\n\u001b[1;32m    324\u001b[0m         norm_hidden_states,\n\u001b[1;32m    325\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    326\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[0;32m--> 329\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# 4. Feed-forward\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm_single:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (49152) must match the size of tensor b (8192) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a pipeline for easier inference\n",
    "pipeline = CubeDiffPipeline(model, device)\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"A beautiful sunset over the ocean with mountains in the distance\"\n",
    "\n",
    "# Generate a panorama\n",
    "print(f\"Generating panorama from prompt: {prompt}\")\n",
    "output = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"blurry, low quality, distorted\",\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    output_type=\"equirectangular\",\n",
    "    height=512,\n",
    "    width=1024,\n",
    ")\n",
    "\n",
    "# Display the output\n",
    "if isinstance(output, torch.Tensor):\n",
    "    # Convert to image\n",
    "    if output.ndim == 4:  # Batch of images\n",
    "        output = output[0]  # Take first image\n",
    "    \n",
    "    # Move to CPU and convert to numpy\n",
    "    output_np = output.cpu().permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "    \n",
    "    # Ensure values are in [0, 1]\n",
    "    if output_np.max() > 1.0:\n",
    "        output_np = output_np / 255.0\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(output_np)\n",
    "    plt.title(f\"Generated Panorama: {prompt}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    # If output is already an image, display it\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(output)\n",
    "    plt.title(f\"Generated Panorama: {prompt}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Image-to-Panorama Generation\n",
    "\n",
    "Now let's test generating a panorama from a normal field-of-view (NFoV) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load a sample input image (normal field-of-view)\n",
    "# Replace this with your own sample image path\n",
    "input_path = \"./images/bridge.jpg\"\n",
    "\n",
    "if os.path.exists(input_path):\n",
    "    # Load the image\n",
    "    input_img = Image.open(input_path).convert(\"RGB\")\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(input_img)\n",
    "    plt.title(\"Input NFoV Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    from torchvision import transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    input_tensor = transform(input_img).unsqueeze(0).to(device)  # (1, 3, 512, 512)\n",
    "    \n",
    "    # Text prompt for conditioning\n",
    "    prompt = \"A wide view of the scene, 360 degrees panorama, high quality\"\n",
    "    \n",
    "    # Generate a panorama from the input image\n",
    "    print(f\"Generating panorama from input image with prompt: {prompt}\")\n",
    "    output = pipeline(\n",
    "        prompt=prompt,\n",
    "        input_image=input_tensor,  # Provide the input image\n",
    "        condition_face=0,  # Use the front face for conditioning\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        output_type=\"equirectangular\",\n",
    "        height=512,\n",
    "        width=1024,\n",
    "    )\n",
    "    \n",
    "    # Display the output\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        # Convert to image\n",
    "        if output.ndim == 4:  # Batch of images\n",
    "            output = output[0]  # Take first image\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        output_np = output.cpu().permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "        \n",
    "        # Ensure values are in [0, 1]\n",
    "        if output_np.max() > 1.0:\n",
    "            output_np = output_np / 255.0\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(15, 7.5))\n",
    "        plt.imshow(output_np)\n",
    "        plt.title(f\"Generated Panorama from Input Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # If output is already an image, display it\n",
    "        plt.figure(figsize=(15, 7.5))\n",
    "        plt.imshow(output)\n",
    "        plt.title(f\"Generated Panorama from Input Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Input image not found at {input_path}. Please provide a valid path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Cubemap Representation\n",
    "\n",
    "Let's visualize the intermediate cubemap representation of the generated panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate a panorama in cubemap format\n",
    "prompt = \"A forest with a river and mountains in the distance\"\n",
    "\n",
    "# Generate a panorama with cubemap output\n",
    "print(f\"Generating cubemap from prompt: {prompt}\")\n",
    "cubemap_output = pipeline(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    output_type=\"cubemap\",  # Output as cubemap\n",
    "    height=256,\n",
    "    width=256,\n",
    ")\n",
    "\n",
    "# Visualize the cubemap\n",
    "if isinstance(cubemap_output, torch.Tensor):\n",
    "    # If batch of cubemaps, take the first one\n",
    "    if cubemap_output.ndim == 5:  # (B, 6, C, H, W)\n",
    "        cubemap_output = cubemap_output[0]  # (6, C, H, W)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_cubemap(cubemap_output, title=f\"Generated Cubemap: {prompt}\")\n",
    "    \n",
    "    # Convert to equirectangular for comparison\n",
    "    equirect = cubemap_to_equirectangular_batch(\n",
    "        cubemap_output.unsqueeze(0).to(device),\n",
    "        height=512,\n",
    "        width=1024\n",
    "    )\n",
    "    \n",
    "    # Display equirectangular\n",
    "    equirect_np = equirect[0].cpu().permute(1, 2, 0).numpy()\n",
    "    if equirect_np.max() > 1.0:\n",
    "        equirect_np = equirect_np / 255.0\n",
    "    \n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(equirect_np)\n",
    "    plt.title(f\"Converted to Equirectangular\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setting Up for Training\n",
    "\n",
    "If you have a dataset of panoramic images, you can train or fine-tune the CubeDiff model. Here's how to set up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Example of setting up a dataset and dataloader\n",
    "# Replace these paths with your actual data\n",
    "panorama_paths = [\n",
    "    \"data/panorama1.jpg\",\n",
    "    \"data/panorama2.jpg\",\n",
    "    # Add more paths...\n",
    "]\n",
    "\n",
    "caption_paths = [\n",
    "    \"data/caption1.txt\",\n",
    "    \"data/caption2.txt\",\n",
    "    # Add more paths...\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "# Note: Only execute this if you have actual data\n",
    "if all(os.path.exists(path) for path in panorama_paths) and all(os.path.exists(path) for path in caption_paths):\n",
    "    # Create train dataset\n",
    "    train_dataset = CubemapDataset(\n",
    "        image_paths=panorama_paths,\n",
    "        caption_paths=caption_paths,\n",
    "        face_size=128,  # Size of each cubemap face\n",
    "    )\n",
    "    \n",
    "    # Create train dataloader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    \n",
    "    # Test the dataset\n",
    "    test_dataset(train_dataset, num_samples=1)\n",
    "    \n",
    "    # Create optimizer and scheduler\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = CubeDiffTrainer(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=\"output\",\n",
    "    )\n",
    "    \n",
    "    print(\"Training setup complete. You can now run trainer, train() to start training.\")\n",
    "else:\n",
    "    print(\"Training dataset not available. Skipping training setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examining the Model Architecture\n",
    "\n",
    "Let's examine the architecture of the CubeDiff model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Print model summary\n",
    "print(f\"CubeDiff Model Architecture Overview:\")\n",
    "print(f\"==================================\")\n",
    "print(f\"Text Encoder: {model.text_encoder.__class__.__name__}\")\n",
    "print(f\"VAE: {model.vae.__class__.__name__}\")\n",
    "print(f\"UNet: {model.unet.__class__.__name__}\")\n",
    "print(f\"Scheduler: {model.scheduler.__class__.__name__}\")\n",
    "print(f\"==================================\")\n",
    "print(f\"Custom Modules:\")\n",
    "print(f\"- CubemapPositionalEncoding\")\n",
    "print(f\"- GroupNormalizationSync\")\n",
    "print(f\"- OverlappingEdgeProcessor\")\n",
    "print(f\"==================================\")\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding the Diffusion Process\n",
    "\n",
    "Let's visualize the diffusion process step by step to understand how the model generates panoramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function to visualize the diffusion process\n",
    "def visualize_diffusion_process(model, prompt, num_inference_steps=20, output_type=\"equirectangular\"):\n",
    "    \"\"\"Visualize the diffusion process step by step.\"\"\"\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Process prompt\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    \n",
    "    # Batch size\n",
    "    batch_size = len(prompt)\n",
    "    \n",
    "    # Encode text\n",
    "    text_embeddings = model.encode_text(prompt)\n",
    "    uncond_embeddings = model.encode_text([\"\" for _ in range(batch_size)])\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    \n",
    "    # Set timesteps\n",
    "    model.scheduler.set_timesteps(num_inference_steps, device=model.device)\n",
    "    timesteps = model.scheduler.timesteps\n",
    "    \n",
    "    # Prepare initial latents\n",
    "    latents = model.prepare_latents(\n",
    "        batch_size=batch_size,\n",
    "        num_faces=6,\n",
    "        height=256,\n",
    "        width=256,\n",
    "    )\n",
    "    \n",
    "    # Apply cubemap-specific processing\n",
    "    latents = model.cubemap_pos_encoding(latents)\n",
    "    latents = model.group_norm_sync(latents)\n",
    "    \n",
    "    # Initialize output list\n",
    "    outputs = []\n",
    "    \n",
    "    # Denoising loop\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # Duplicate for classifier-free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        \n",
    "        # Scale according to the scheduler\n",
    "        latent_model_input = model.scheduler.scale_model_input(latent_model_input, t)\n",
    "        \n",
    "        # Reshape for UNet\n",
    "        b, f, c, h, w = latent_model_input.shape\n",
    "        reshaped_input = latent_model_input.view(b * f, c, h, w)\n",
    "        \n",
    "        # Predict noise\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model.unet(reshaped_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "            noise_pred = noise_pred.view(b, f, c, h, w)\n",
    "        \n",
    "        # Apply classifier-free guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + 7.5 * (noise_pred_text - noise_pred_uncond)\n",
    "        \n",
    "        # Scheduler step\n",
    "        latents = model.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # Apply cubemap-specific processing\n",
    "        latents = model.cubemap_pos_encoding(latents)\n",
    "        latents = model.group_norm_sync(latents)\n",
    "        \n",
    "        # Decode the current latents and add to outputs\n",
    "        if i % (num_inference_steps // 5) == 0 or i == num_inference_steps - 1:\n",
    "            # Decode current state\n",
    "            current_output = model.decode_latents_to_image(latents)\n",
    "            \n",
    "            # Convert to equirectangular if needed\n",
    "            if output_type == \"equirectangular\":\n",
    "                current_output = cubemap_to_equirectangular_batch(\n",
    "                    current_output,\n",
    "                    height=512,\n",
    "                    width=1024\n",
    "                )\n",
    "            \n",
    "            outputs.append({\n",
    "                \"step\": i,\n",
    "                \"timestep\": t.item(),\n",
    "                \"output\": current_output\n",
    "            })\n",
    "    \n",
    "    # Plot all outputs\n",
    "    num_outputs = len(outputs)\n",
    "    fig, axes = plt.subplots(\n",
    "        1, num_outputs, \n",
    "        figsize=(5 * num_outputs, 5),\n",
    "        squeeze=False\n",
    "    )\n",
    "    \n",
    "    for i, output_dict in enumerate(outputs):\n",
    "        step = output_dict[\"step\"]\n",
    "        timestep = output_dict[\"timestep\"]\n",
    "        output = output_dict[\"output\"]\n",
    "        \n",
    "        # Convert to numpy\n",
    "        if output_type == \"equirectangular\":\n",
    "            # Take first image from batch\n",
    "            img = output[0].cpu().permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            # For cubemap visualization\n",
    "            img = output[0]  # Will handle cubemap visualization separately\n",
    "        \n",
    "        # Ensure values are in [0, 1]\n",
    "        if isinstance(img, np.ndarray) and img.max() > 1.0:\n",
    "            img = img / 255.0\n",
    "        \n",
    "        # Display\n",
    "        if output_type == \"equirectangular\":\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title(f\"Step {step}\\nTimestep {timestep}\")\n",
    "            axes[0, i].axis('off')\n",
    "        else:\n",
    "            # For cubemap, display a separate visualization\n",
    "            if i == 0:\n",
    "                # Only do the first one here, we'll handle the rest separately\n",
    "                axes[0, i].imshow(np.zeros((10, 10, 3)))  # Placeholder\n",
    "                axes[0, i].set_title(f\"See separate cubemap visualizations\")\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Visualize each cubemap separately\n",
    "                for j, output_dict in enumerate(outputs):\n",
    "                    visualize_cubemap(\n",
    "                        output_dict[\"output\"][0],\n",
    "                        title=f\"Step {output_dict['step']}, Timestep {output_dict['timestep']}\"\n",
    "                    )\n",
    "            else:\n",
    "                axes[0, i].axis('off')  # Hide other axes\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Now let's visualize the diffusion process\n",
    "prompt = \"A mountains landscape with a lake\"\n",
    "\n",
    "# Run the visualization\n",
    "try:\n",
    "    diffusion_steps = visualize_diffusion_process(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=20,\n",
    "        num_inference_steps=20,\n",
    "        output_type=\"equirectangular\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing diffusion process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the CubeDiff model for generating high-quality 360° panoramas. The key advantages of this approach include:\n",
    "\n",
    "1. **Reusing existing diffusion models**: By adapting pretrained text-to-image diffusion models, CubeDiff leverages the capabilities of existing models without training from scratch.\n",
    "\n",
    "2. **Cubemap representation**: Using cubemap instead of equirectangular projection reduces distortions and makes it easier to generate consistent panoramas.\n",
    "\n",
    "3. **Specialized components for cubemap processing**:\n",
    "   - Cubemap positional encoding for spatial understanding\n",
    "   - Synchronized group normalization for consistent colors\n",
    "   - Overlapping edge processor for seamless transitions\n",
    "   - Inflated attention for cross-face relationships\n",
    "\n",
    "4. **Flexible conditioning**: The model supports both text-to-panorama and image-to-panorama generation.\n",
    "\n",
    "The implementation is structured in a modular way, making it easy to understand, modify, and extend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
