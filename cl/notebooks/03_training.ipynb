{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-4-20 run it in \"pytohn 3 base env\" on ins-gl-pt-gpu24-2c94136-3env-j4-l4-test-1 with 1 L4 GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeDiff: Training Pipeline\n",
    "\n",
    "This notebook demonstrates the training pipeline for CubeDiff:\n",
    "\n",
    "1. Configure training parameters\n",
    "2. Initialize the model and training components\n",
    "3. Train on a small dataset\n",
    "4. Monitor training progress\n",
    "5. Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers==0.24.0\n",
      "  Using cached diffusers-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting torch==2.1.2\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision==0.16.2\n",
      "  Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting accelerate==0.25.0\n",
      "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting opencv-python==4.8.1.78\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting opencv-contrib-python==4.8.1.78\n",
      "  Using cached opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib==3.8.2\n",
      "  Using cached matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting tqdm==4.66.1\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting einops==0.7.0\n",
      "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting huggingface_hub==0.19.4\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting xformers\n",
      "  Using cached xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (11.0.0)\n",
      "Collecting openexr\n",
      "  Using cached openexr-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.24.0) (3.16.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.24.0) (8.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.24.0) (1.25.2)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.24.0)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.24.0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2)\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (5.9.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (2.9.0.post0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.4.99)\n",
      "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xformers\n",
      "  Using cached xformers-0.0.29.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.28-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "INFO: pip is still looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.27.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached xformers-0.0.25-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Using cached xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.10.19)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.26.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.24.0) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Using cached diffusers-0.24.0-py3-none-any.whl (1.8 MB)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "Using cached accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "Using cached opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
      "Using cached matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "Using cached openexr-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Using cached wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached sentry_sdk-2.26.1-py2.py3-none-any.whl (340 kB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Installing collected packages: triton, tqdm, setproctitle, sentry-sdk, safetensors, regex, openexr, opencv-python-headless, opencv-python, opencv-contrib-python, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, docker-pycreds, nvidia-cusolver-cu12, nvidia-cudnn-cu12, matplotlib, huggingface_hub, wandb, torch, tokenizers, diffusers, xformers, transformers, torchvision, accelerate\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.99\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.3\n",
      "    Uninstalling matplotlib-3.7.3:\n",
      "      Successfully uninstalled matplotlib-3.7.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0+cu124\n",
      "    Uninstalling torch-2.4.0+cu124:\n",
      "      Successfully uninstalled torch-2.4.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.0+cu124\n",
      "    Uninstalling torchvision-0.19.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.19.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.6.0 requires matplotlib<=3.7.3,>=3.2, but you have matplotlib 3.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.25.0 diffusers-0.24.0 docker-pycreds-0.4.0 einops-0.7.0 huggingface_hub-0.19.4 matplotlib-3.8.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 opencv-contrib-python-4.8.1.78 opencv-python-4.8.1.78 opencv-python-headless-4.11.0.86 openexr-3.3.3 regex-2024.11.6 safetensors-0.5.3 sentry-sdk-2.26.1 setproctitle-1.3.5 tokenizers-0.15.2 torch-2.1.2 torchvision-0.16.2 tqdm-4.66.1 transformers-4.36.2 triton-2.1.0 wandb-0.19.9 xformers-0.0.23.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install diffusers==0.24.0 transformers==4.36.2 torch==2.1.2 torchvision==0.16.2 accelerate==0.25.0 \\\n",
    "    opencv-python==4.8.1.78 opencv-contrib-python==4.8.1.78 matplotlib==3.8.2 tqdm==4.66.1 einops==0.7.0 \\\n",
    "    huggingface_hub==0.19.4 opencv-python xformers requests pillow openexr opencv-python-headless wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/jupyter/mluser/git/llm-cv-pano-cubediff to Python path\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb initialized in offline mode\n",
      "Configuring system for CubeDiff training...\n",
      "System: Linux\n",
      "Python: 3.10.16\n",
      "CPU cores: 16\n",
      "GPU: NVIDIA L4 x1\n",
      "GPU memory: 22.0 GB\n",
      "CUDA version: 12.1\n",
      "\n",
      "Shared memory status:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "shm              64M     0   64M   0% /dev/shm\n",
      "\n",
      "Current shared memory size: 64M\n",
      "\n",
      "⚠️ WARNING: Shared memory size is small!\n",
      "To increase shared memory (requires sudo):\n",
      "  sudo mount -o remount,size=16G /dev/shm\n",
      "\n",
      "If you encounter 'bus error' or 'shared memory' errors during training:\n",
      "1. Consider increasing shared memory with the command above\n",
      "2. Reduce batch_size and num_workers in your config\n",
      "3. Use persistent_workers=True in DataLoader\n",
      "\n",
      "Detected NVIDIA L4 GPU - optimizing settings specifically for this hardware\n",
      "\n",
      "Optimized settings for NVIDIA L4 GPU on GCP Vertex AI:\n",
      "  batch_size: 2\n",
      "  gradient_accumulation_steps: 4\n",
      "  mixed_precision: fp16\n",
      "  num_workers: 1\n",
      "  (Shared memory available: 0.1 GB)\n",
      "  ⚠️ Warning: Limited shared memory detected. Consider increasing with:\n",
      "    sudo mount -o remount,size=16G /dev/shm\n",
      "\n",
      "Recommended settings for full training:\n",
      "  batch_size: 4\n",
      "  gradient_accumulation_steps: 2\n",
      "  num_workers: 2\n",
      "\n",
      "Recommended settings for mini-training:\n",
      "  batch_size: 2\n",
      "  num_workers: 1\n",
      "  gradient_accumulation_steps: 4\n",
      "  mixed_precision: fp16\n",
      "\n",
      "Progressive loading settings (starts safe, gradually increases):\n",
      "  initial_batch_size: 1\n",
      "  initial_num_workers: 0\n",
      "  target_batch_size: 2\n",
      "  target_num_workers: 2\n",
      "  gradient_accumulation_steps: 4\n",
      "  mixed_precision: fp16\n",
      "\n",
      "PyTorch multiprocessing:\n",
      "  sharing strategy: file_system\n",
      "  start method: spawn\n",
      "\n",
      "CUDA cache cleared\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"03_training.ipynb\"\n",
    "\n",
    "# Add parent directory to path\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "# # Import custom modules\n",
    "# from model.architecture import CubeDiffModel\n",
    "# from data.dataset import CubemapDataset, get_dataloader\n",
    "# from training.trainer import CubeDiffTrainer\n",
    "# from training.lora import add_lora_to_model\n",
    "\n",
    "\n",
    "# Add the git repository location directly\n",
    "repo_path = '/home/jupyter/mluser/git/llm-cv-pano-cubediff'\n",
    "if os.path.exists(repo_path) and repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "    print(f\"Added {repo_path} to Python path\")\n",
    "\n",
    "# Initialize wandb in offline mode\n",
    "wandb.init(mode=\"offline\", project=\"cubediff_mini\")\n",
    "print(\"Wandb initialized in offline mode\")\n",
    "\n",
    "# Import the modules directly with fixed imports\n",
    "# These imports now use the absolute path style\n",
    "from cl.model.architecture import CubeDiffModel\n",
    "from cl.data.dataset import CubemapDataset, get_dataloader\n",
    "from cl.training.trainer import CubeDiffTrainer, get_tensor_info\n",
    "from cl.training.lora import add_lora_to_model\n",
    "# Import the system configuration utility\n",
    "from cl.training.system_config import configure_system_for_training\n",
    "# Import the adaptive function (if not added to system_config.py)\n",
    "from cl.training.system_config import adaptive_shm_handling, configure_system_for_training, print_model_size\n",
    "from cl.training.trainer import add_memory_monitoring\n",
    "from cl.training.fix_mixed_precision import fix_mixed_precision_issue\n",
    "\n",
    "# Run system configuration and get recommended settings\n",
    "recommended_settings = configure_system_for_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Configuration:\n",
      "Batch Size: 2\n",
      "Workers: 2\n",
      "Gradient Accumulation Steps: 4\n",
      "Mixed Precision: fp16\n"
     ]
    }
   ],
   "source": [
    "# Create a configuration class\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model config\n",
    "        self.pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.lora_rank = 16\n",
    "        self.lora_alpha = 16\n",
    "        self.prediction_type = \"v_prediction\"  # or \"epsilon\"\n",
    "        \n",
    "        # Training config - optimized for L4 GPU\n",
    "        self.output_dir = \"../outputs/cubediff_mini\"\n",
    "        self.data_dir = \"../data/processed/cubemaps\"\n",
    "        self.captions_file = \"../data/processed/captions.json\"\n",
    "        self.batch_size = 32  # L4 GPU should handle batch size of 2\n",
    "        self.learning_rate = 1e-4\n",
    "        self.min_learning_rate = 1e-6\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_workers = 14  # Start with 2 workers for multiprocessing\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.mixed_precision = \"fp16\"  # Keep fp16 for L4 GPU efficiency\n",
    "        # Example offloading settings (syntax may vary based on the framework)\n",
    "        self.offload_optimizer = True  # Offload optimizer states to CPU, Enable offloading for L4 GPU\n",
    "        self.offload_parameters = True  # Offload some parameters to CPU when not in use\n",
    "        # Memory optimization flags\n",
    "        # self.use_attention_mask = False  # If applicable for your model\n",
    "        self.use_memory_efficient_attention = True  # If supported by your model architecture\n",
    "        self.gradient_checkpointing = True  # Enable gradient checkpointing to save memory\n",
    "\n",
    "        # Logging config\n",
    "        self.use_wandb = True  # Enable wandb\n",
    "        self.wandb_project = \"cubediff\"\n",
    "        self.wandb_run_name = \"cubediff_mini\"\n",
    "        self.log_every_n_steps = 10\n",
    "        self.save_every_n_steps = 100\n",
    "        self.eval_every_n_steps = 100\n",
    "        self.sample_every_n_steps = 250\n",
    "        \n",
    "        # Progressive loading (helps detect and avoid memory issues early)\n",
    "        self.progressive_loading = True\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Print optimized configuration\n",
    "print(\"\\nOptimized Configuration:\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Workers: {config.num_workers}\")\n",
    "print(f\"Gradient Accumulation Steps: {config.gradient_accumulation_steps}\")\n",
    "print(f\"Mixed Precision: {config.mixed_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shared Memory Status:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "shm              64M     0   64M   0% /dev/shm\n",
      "\n",
      "Available shared memory: 0.06 GB\n",
      "System has 16 CPU cores\n",
      "Recommended workers based on system: 0\n",
      "Adjusting workers from 2 to 0 based on system resources\n"
     ]
    }
   ],
   "source": [
    "def check_shared_memory():\n",
    "    \"\"\"\n",
    "    Check available shared memory and provide recommendations for\n",
    "    optimal multiprocessing settings.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (shm_size_gb, recommended_workers)\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Default recommended values if we can't check\n",
    "    recommended_workers = 2\n",
    "    shm_size_gb = None\n",
    "    \n",
    "    try:\n",
    "        # Check shared memory size on Linux\n",
    "        result = subprocess.run(['df', '-h', '/dev/shm'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            output = result.stdout.decode('utf-8')\n",
    "            print(\"\\nShared Memory Status:\")\n",
    "            print(output)\n",
    "            \n",
    "            # Extract shared memory size\n",
    "            lines = output.strip().split('\\n')\n",
    "            if len(lines) > 1:\n",
    "                parts = lines[1].split()\n",
    "                if len(parts) >= 2:\n",
    "                    shm_size = parts[1]\n",
    "                    if 'G' in shm_size:\n",
    "                        shm_size_gb = float(shm_size.replace('G', ''))\n",
    "                    elif 'M' in shm_size:\n",
    "                        shm_size_gb = float(shm_size.replace('M', '')) / 1024\n",
    "                    \n",
    "                    print(f\"Available shared memory: {shm_size_gb:.2f} GB\")\n",
    "                    \n",
    "                    # Adjust workers based on shared memory\n",
    "                    if shm_size_gb >= 16:\n",
    "                        recommended_workers = 4  # Plenty of shared memory\n",
    "                    elif shm_size_gb >= 8:\n",
    "                        recommended_workers = 2  # Moderate shared memory\n",
    "                    elif shm_size_gb >= 4:\n",
    "                        recommended_workers = 1  # Limited shared memory\n",
    "                    else:\n",
    "                        recommended_workers = 0  # Very limited shared memory\n",
    "    except Exception as e:\n",
    "        print(f\"Could not check shared memory: {e}\")\n",
    "    \n",
    "    # Consider CPU count as well\n",
    "    import os\n",
    "    cpu_count = os.cpu_count()\n",
    "    cpu_recommended = max(1, min(4, cpu_count // 4))\n",
    "    \n",
    "    # Take the minimum of both constraints\n",
    "    final_recommended = min(recommended_workers, cpu_recommended)\n",
    "    \n",
    "    print(f\"System has {cpu_count} CPU cores\")\n",
    "    print(f\"Recommended workers based on system: {final_recommended}\")\n",
    "    \n",
    "    if shm_size_gb is not None and shm_size_gb < 8 and final_recommended > 0:\n",
    "        print(\"\\nWarning: Limited shared memory detected.\")\n",
    "        print(\"If you encounter 'bus error' or 'shared memory' errors, try:\")\n",
    "        print(\"1. Reduce num_workers in your config\")\n",
    "        print(\"2. Increase shared memory with: sudo mount -o remount,size=16G /dev/shm\")\n",
    "    \n",
    "    return shm_size_gb, final_recommended\n",
    "\n",
    "# Run the shared memory check\n",
    "shm_size_gb, recommended_workers = check_shared_memory()\n",
    "\n",
    "# Update config with recommended workers if needed\n",
    "if hasattr(config, 'num_workers') and config.num_workers > recommended_workers:\n",
    "    print(f\"Adjusting workers from {config.num_workers} to {recommended_workers} based on system resources\")\n",
    "    config.num_workers = recommended_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check\n",
    "\n",
    "Let's confirm that we have the required resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA L4\n",
      "GPU memory: 22.05 GB\n",
      "Number of GPUs: 1\n",
      "Data directory exists: True\n",
      "Output directory: ../outputs/cubediff_mini\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024:.2f} GB\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"GPU not available, training will be slow on CPU.\")\n",
    "\n",
    "# Check directory structure\n",
    "data_path_exists = os.path.exists(config.data_dir)\n",
    "print(f\"Data directory exists: {data_path_exists}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 5\n",
      "Sample caption: A well-lit artist workshop with painting equipment, natural lighting from windows, and various art tools arranged throughout the space\n",
      "Sample faces shape: torch.Size([6, 3, 512, 512])\n",
      "Sample faces dtype: torch.float32\n",
      "Sample faces min/max: 0.0/1.0\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = CubemapDataset(\n",
    "    data_dir=config.data_dir,\n",
    "    captions_file=config.captions_file\n",
    ")\n",
    "\n",
    "# For this mini example, we'll use the same dataset for validation\n",
    "val_dataset = train_dataset\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Set the number of steps for mini-training\n",
    "num_steps = 10 # 250  # Just for demonstration\n",
    "\n",
    "# Inspect a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample caption: {sample['caption']}\")\n",
    "print(f\"Sample faces shape: {sample['faces'].shape}\")\n",
    "print(f\"Sample faces dtype: {sample['faces'].dtype}\")\n",
    "print(f\"Sample faces min/max: {sample['faces'].min()}/{sample['faces'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VAE compatibility with proper dtype handling\n",
    "def test_vae_compatibility(trainer):\n",
    "    \"\"\"\n",
    "    Test VAE encoder compatibility with different tensor formats\n",
    "    and ensure proper dtype handling\n",
    "    \"\"\"\n",
    "    # Get VAE parameters\n",
    "    vae_param = next(trainer.vae.parameters())\n",
    "    vae_dtype = vae_param.dtype\n",
    "    vae_device = vae_param.device\n",
    "    \n",
    "    print(f\"VAE is using dtype: {vae_dtype} on device: {vae_device}\")\n",
    "    \n",
    "    # Test NCHW format (channels first)\n",
    "    print(\"\\nTesting NCHW format (channels first):\")\n",
    "    test_input = torch.randn(1, 3, 512, 512, dtype=vae_dtype, device=vae_device)\n",
    "    print(f\"Test tensor shape: {test_input.shape}, dtype: {test_input.dtype}\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            latent = trainer.vae.encode(test_input).latent_dist.sample()\n",
    "            print(\"✓ VAE accepts NCHW format\")\n",
    "            print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ VAE error with NCHW format: {e}\")\n",
    "    \n",
    "    # Test NHWC format (channels last)\n",
    "    print(\"\\nTesting NHWC format (channels last):\")\n",
    "    test_input = torch.randn(1, 512, 512, 3)  # Start with FP32\n",
    "    print(f\"Original tensor shape: {test_input.shape}, dtype: {test_input.dtype}\")\n",
    "    \n",
    "    # Convert to VAE dtype and permute to NCHW\n",
    "    test_input = test_input.permute(0, 3, 1, 2).to(dtype=vae_dtype, device=vae_device)\n",
    "    print(f\"Converted tensor shape: {test_input.shape}, dtype: {test_input.dtype}\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            latent = trainer.vae.encode(test_input).latent_dist.sample()\n",
    "            print(\"✓ VAE accepts converted NHWC->NCHW format\")\n",
    "            print(f\"Encoded latent shape: {latent.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ VAE error with converted format: {e}\")\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_noise_scheduler(trainer):\n",
    "    \"\"\"\n",
    "    Ensure the noise scheduler handles FP16 inputs correctly by patching\n",
    "    its critical methods to work properly with the training loop.\n",
    "    \"\"\"\n",
    "    # Backup original methods\n",
    "    original_add_noise = trainer.noise_scheduler.add_noise\n",
    "    original_get_velocity = trainer.noise_scheduler.get_velocity\n",
    "    \n",
    "    # Get the model dtype (should be FP16 for mixed precision)\n",
    "    model_dtype = next(trainer.model.parameters()).dtype\n",
    "    print(f\"Model dtype for patching noise scheduler: {model_dtype}\")\n",
    "    \n",
    "    # Create patched versions of the methods\n",
    "    def patched_add_noise(original_samples, noise, timesteps):\n",
    "        \"\"\"Process without changing input/output types\"\"\"\n",
    "        # Debug info\n",
    "        get_tensor_info(original_samples, \"add_noise input samples\")\n",
    "        get_tensor_info(noise, \"add_noise input noise\")\n",
    "        get_tensor_info(timesteps, \"add_noise input timesteps\")\n",
    "        \n",
    "        # Ensure the timesteps are on the right device\n",
    "        timesteps_device = timesteps.to(device=original_samples.device)\n",
    "        \n",
    "        try:\n",
    "            # Call original method with inputs as-is (already float32 from training loop)\n",
    "            result = original_add_noise(original_samples, noise, timesteps_device)\n",
    "            \n",
    "            # Debug result\n",
    "            get_tensor_info(result, \"add_noise result\")\n",
    "            \n",
    "            # Return without changing the dtype\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in add_noise: {e}\")\n",
    "            print(\"Trying with explicit float32...\")\n",
    "            \n",
    "            # Last resort\n",
    "            return original_add_noise(\n",
    "                original_samples.float(), \n",
    "                noise.float(),\n",
    "                timesteps_device\n",
    "            )\n",
    "    \n",
    "    def patched_get_velocity(sample, noise, timesteps):\n",
    "        \"\"\"Process without changing input/output types\"\"\"\n",
    "        # Debug info\n",
    "        get_tensor_info(sample, \"get_velocity input sample\")\n",
    "        get_tensor_info(noise, \"get_velocity input noise\")\n",
    "        get_tensor_info(timesteps, \"get_velocity input timesteps\")\n",
    "        \n",
    "        # Ensure the timesteps are on the right device\n",
    "        timesteps_device = timesteps.to(device=sample.device)\n",
    "        \n",
    "        try:\n",
    "            # Call original method with inputs as-is (already float32 from training loop)\n",
    "            result = original_get_velocity(sample, noise, timesteps_device)\n",
    "            \n",
    "            # Debug result\n",
    "            get_tensor_info(result, \"get_velocity result\")\n",
    "            \n",
    "            # Return without changing the dtype\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_velocity: {e}\")\n",
    "            print(\"Trying with explicit float32...\")\n",
    "            \n",
    "            # Last resort\n",
    "            return original_get_velocity(\n",
    "                sample.float(), \n",
    "                noise.float(),\n",
    "                timesteps_device\n",
    "            )\n",
    "    \n",
    "    # Replace original methods with patched versions\n",
    "    trainer.noise_scheduler.add_noise = patched_add_noise\n",
    "    trainer.noise_scheduler.get_velocity = patched_get_velocity\n",
    "\n",
    "    print(\"Noise scheduler patched to handle mixed precision correctly\")\n",
    "    return trainer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to a new cell before running the training\n",
    "def patch_trainer_for_mixed_precision(trainer):\n",
    "    \"\"\"\n",
    "    Patch the trainer to properly handle mixed precision training\n",
    "    by ensuring model parameters that need gradients are in float32.\n",
    "    \"\"\"\n",
    "    # First, ensure the model's parameters that require gradients are float32\n",
    "    # This is critical for the optimizer to work properly with gradient scaling\n",
    "    device = trainer.accelerator.device if hasattr(trainer, 'accelerator') else \"cuda\"\n",
    "    \n",
    "    print(\"Converting trainable parameters to float32 for stable mixed precision training...\")\n",
    "    for name, param in trainer.model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Keep trainable params in float32 for optimizer\n",
    "            param.data = param.data.to(dtype=torch.float32)\n",
    "    \n",
    "    # Patch the train method to handle the optimizer initialization correctly\n",
    "    original_train = trainer.train\n",
    "    \n",
    "    def patched_train(self, train_dataset, val_dataset=None, num_train_epochs=30000):\n",
    "        print(\"Using patched training method for stable mixed precision...\")\n",
    "        \n",
    "        # Empty CUDA cache before starting\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Prepare dataloaders\n",
    "        train_dataloader, val_dataloader = self.prepare_dataloaders(train_dataset, val_dataset)\n",
    "        \n",
    "        # Collect trainable parameters properly and ensure they're float32\n",
    "        trainable_params = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Verify it's float32 and on the correct device\n",
    "                if param.dtype != torch.float32:\n",
    "                    param.data = param.data.to(dtype=torch.float32)\n",
    "                trainable_params.append(param)\n",
    "        \n",
    "        # Initialize optimizer with properly collected parameters\n",
    "        optimizer = AdamW(\n",
    "            trainable_params,\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            betas=(0.9, 0.999),\n",
    "        )\n",
    "        \n",
    "        # Set up scheduler\n",
    "        lr_scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=num_train_epochs,\n",
    "            eta_min=self.config.min_learning_rate,\n",
    "        )\n",
    "        \n",
    "        # Prepare everything with accelerator\n",
    "        self.model, optimizer, train_dataloader, lr_scheduler = self.accelerator.prepare(\n",
    "            self.model, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "        \n",
    "        if val_dataloader:\n",
    "            val_dataloader = self.accelerator.prepare(val_dataloader)\n",
    "        \n",
    "        # Continue with the rest of the original train method logic\n",
    "        # by calling the remaining implementation through a partial function\n",
    "        from functools import partial\n",
    "        \n",
    "        # Create a partial function that skips the parts we've already handled\n",
    "        remaining_impl = partial(\n",
    "            original_train,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            _skip_initialization=True  # Add this flag to signal skipping initialization\n",
    "        )\n",
    "        \n",
    "        # Add the _skip_initialization handling to the original train method if not there\n",
    "        if not hasattr(original_train, '_patched'):\n",
    "            def wrapped_original_train(self, train_dataset, val_dataset=None, num_train_epochs=30000, _skip_initialization=False):\n",
    "                if _skip_initialization:\n",
    "                    # Skip the initialization part and go straight to training loop\n",
    "                    # The code here should duplicate the training loop part of the original train method\n",
    "                    # but skip the dataloader and optimizer initialization\n",
    "                    \n",
    "                    # ... Training loop from original goes here ...\n",
    "                    # This is a simplified placeholder - in practice we'd need to extract the loop\n",
    "                    print(\"Skipped initialization, proceeding to training loop...\")\n",
    "                else:\n",
    "                    # Call the true original implementation\n",
    "                    return original_train(self, train_dataset, val_dataset, num_train_epochs)\n",
    "            \n",
    "            # Replace with wrapped version\n",
    "            wrapped_original_train._patched = True\n",
    "            trainer.train = types.MethodType(wrapped_original_train, trainer)\n",
    "        \n",
    "        # Call the original train method with our initialized components\n",
    "        return remaining_impl()\n",
    "    \n",
    "    # Replace the train method\n",
    "    trainer.train = types.MethodType(patched_train, trainer)\n",
    "    \n",
    "    # Also patch the ensure_dtype method to handle gradients properly\n",
    "    def patched_ensure_dtype(self, tensor, reference_tensor=None):\n",
    "        \"\"\"\n",
    "        Ensure tensor has correct dtype for different operations:\n",
    "        - For parameters with gradients: float32\n",
    "        - For activations/intermediate values: mixed precision (float16/float32)\n",
    "        \"\"\"\n",
    "        if tensor.requires_grad:\n",
    "            # Parameters that need gradients should be float32\n",
    "            return tensor.to(dtype=torch.float32)\n",
    "        elif reference_tensor is not None:\n",
    "            # Match reference tensor dtype\n",
    "            return tensor.to(dtype=reference_tensor.dtype)\n",
    "        else:\n",
    "            # Use model dtype for activations (usually float16 in mixed precision)\n",
    "            model_dtype = next(self.model.parameters()).dtype\n",
    "            return tensor.to(dtype=model_dtype)\n",
    "    \n",
    "    # Replace ensure_dtype method\n",
    "    trainer.ensure_dtype = types.MethodType(patched_ensure_dtype, trainer)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting num_workers to 2\n",
      "DataLoader configured with 2 workers\n",
      "Sharing strategy: file_system\n",
      "Multiprocessing start method: spawn\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync /home/jupyter/mluser/git/llm-cv-pano-cubediff/cl/notebooks/wandb/offline-run-20250422_051407-czvfcbld<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20250422_051407-czvfcbld/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb initialized in offline mode. Logs will be saved to ../outputs/cubediff_mini/logs/wandb\n",
      "To view logs in JupyterLab, use: wandb.jupyter.show()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fc72a57ad747cf97041df43b272ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 861,916,740\n",
      "Trainable parameters: 768,380,100\n",
      "LoRA parameters: 2,390,016\n",
      "VAE dtype: torch.float16\n",
      "Text encoder dtype: torch.float16\n",
      "Model dtype: torch.float32\n",
      "VAE is using dtype: torch.float16 on device: cuda:0\n",
      "\n",
      "Testing NCHW format (channels first):\n",
      "Test tensor shape: torch.Size([1, 3, 512, 512]), dtype: torch.float16\n",
      "✓ VAE accepts NCHW format\n",
      "Encoded latent shape: torch.Size([1, 4, 64, 64])\n",
      "\n",
      "Testing NHWC format (channels last):\n",
      "Original tensor shape: torch.Size([1, 512, 512, 3]), dtype: torch.float32\n",
      "Converted tensor shape: torch.Size([1, 3, 512, 512]), dtype: torch.float16\n",
      "✓ VAE accepts converted NHWC->NCHW format\n",
      "Encoded latent shape: torch.Size([1, 4, 64, 64])\n",
      "VAE is using dtype: torch.float16 on device: cuda:0\n",
      "Test tensor dtype: torch.float16\n",
      "VAE error with NCHW format: 'AutoencoderKLOutput' object has no attribute 'dtype'\n",
      "VAE error with NHWC format: Given groups=1, weight of size [128, 3, 3, 3], expected input[1, 512, 512, 3] to have 3 channels, but got 512 channels instead\n",
      "vae dtype is : torch.float16 and test_input dtype is torch.float16\n",
      "latent dtype : torch.float32\n",
      "LoRA dtype   : torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = CubeDiffTrainer(\n",
    "    config=config,\n",
    "    pretrained_model_name=config.pretrained_model_name,\n",
    "    output_dir=config.output_dir,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# Apply the fix to our trainer\n",
    "trainer = fix_mixed_precision_issue(trainer)\n",
    "\n",
    "# Test VAE compatibility with proper dtype handling\n",
    "test_vae_compatibility(trainer)\n",
    "\n",
    "# Get the VAE's precision to create matching test tensor\n",
    "vae_param = next(trainer.vae.parameters())\n",
    "vae_dtype = vae_param.dtype\n",
    "vae_device = vae_param.device\n",
    "\n",
    "print(f\"VAE is using dtype: {vae_dtype} on device: {vae_device}\")\n",
    "\n",
    "# Create test tensor with matching precision\n",
    "test_input = torch.randn(1, 3, 512, 512, dtype=vae_dtype, device=vae_device)\n",
    "print(f\"Test tensor dtype: {test_input.dtype}\")\n",
    "\n",
    "# Try NCHW format with matching precision\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        latent = trainer.vae.encode(test_input)\n",
    "        print(f\"VAE accepts NCHW format (channels first) with latent dtype is {latent.dtype}\")\n",
    "        print(f\"Encoded latent shape: {latent.latent_dist.sample().shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"VAE error with NCHW format: {e}\")\n",
    "    \n",
    "    # Try NHWC format with matching precision\n",
    "    test_input = torch.randn(1, 512, 512, 3, dtype=vae_dtype, device=vae_device)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            latent = trainer.vae.encode(test_input)\n",
    "            print(\"VAE accepts NHWC format (channels last)\")\n",
    "            print(f\"Encoded latent shape: {latent.latent_dist.sample().shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"VAE error with NHWC format: {e}\")\n",
    "\n",
    "# quick regression test inside 03_training.ipynb\n",
    "test_input = torch.randn(1, 3, 512, 512,\n",
    "                         dtype=next(trainer.vae.parameters()).dtype,\n",
    "                         device=next(trainer.vae.parameters()).device)\n",
    "\n",
    "print(f\"vae dtype is : {next(trainer.vae.parameters()).dtype} and test_input dtype is {test_input.dtype}\")\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    latent = trainer.vae.encode(test_input).latent_dist.sample()\n",
    "print(\"latent dtype :\", latent.dtype)  # should be torch.float16\n",
    "print(\"LoRA dtype   :\", next(iter(trainer.lora_params)).dtype)  # should also be torch.float16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Mini-Training\n",
    "\n",
    "For this demonstration, we'll run a small number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving components to device cuda with dtype torch.float16\n",
      "Model components updated for consistent dtypes\n",
      "VAE device: cuda:0, dtype: torch.float16\n",
      "Text encoder device: cuda:0, dtype: torch.float16\n",
      "Model device: cuda:0, dtype: torch.float16\n",
      "Progressive loading enabled. Starting with batch_size=1, num_workers=0\n",
      "Will gradually increase to batch_size=2, num_workers=2\n",
      "\n",
      "Training with optimized settings for GCP Vertex AI with L4 GPU:\n",
      "- Number of workers: 0\n",
      "- Batch size: 1\n",
      "- Gradient accumulation steps: 4\n",
      "- Mixed precision: fp16\n",
      "- Available shared memory: 0.06 GB\n",
      "Available shared memory: 0.06 GB\n",
      "Adjusting training parameters based on shared memory:\n",
      "  - Original num_workers: 0\n",
      "  - Adjusted num_workers: 0\n",
      "  - Original batch_size: 1\n",
      "  - Adjusted batch_size: 1\n",
      "Model dtype for patching noise scheduler: torch.float16\n",
      "Noise scheduler patched to handle mixed precision correctly\n",
      "Progressive loading enabled. Starting with batch_size=1, num_workers=0\n",
      "Will gradually increase to batch_size=1, num_workers=0\n",
      "03 trainer.train() started ...\n",
      " \n",
      "Using patched train method to fix mixed precision issues\n",
      "DataLoader configured with 0 workers\n",
      "Batch size: 1\n",
      "Prefetch factor: 2\n",
      "Gradient accumulation steps: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ce81b7d3a84a96b23e26d5c2e6a65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched training loop started with 10 steps...\n",
      "add_noise input samples - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "add_noise input samples - Min: -4.984375, Max: 7.83203125\n",
      "add_noise input samples - Requires grad: False\n",
      "add_noise input noise - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "add_noise input noise - Min: -4.891749382019043, Max: 4.069705486297607\n",
      "add_noise input noise - Requires grad: False\n",
      "add_noise input timesteps - Shape: torch.Size([1]), Dtype: torch.int64, Device: cuda:0\n",
      "add_noise input timesteps - Min: 975, Max: 975\n",
      "add_noise input timesteps - Requires grad: False\n",
      "add_noise result - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "add_noise result - Min: -4.945559024810791, Max: 4.2068328857421875\n",
      "add_noise result - Requires grad: False\n",
      "get_velocity input sample - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "get_velocity input sample - Min: -4.984375, Max: 7.83203125\n",
      "get_velocity input sample - Requires grad: False\n",
      "get_velocity input noise - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "get_velocity input noise - Min: -4.891749382019043, Max: 4.069705486297607\n",
      "get_velocity input noise - Requires grad: False\n",
      "get_velocity input timesteps - Shape: torch.Size([1]), Dtype: torch.int64, Device: cuda:0\n",
      "get_velocity input timesteps - Min: 975, Max: 975\n",
      "get_velocity input timesteps - Requires grad: False\n",
      "get_velocity result - Shape: torch.Size([1, 6, 4, 64, 64]), Dtype: torch.float32, Device: cuda:0\n",
      "get_velocity result - Min: -7.7933149337768555, Max: 4.8686089515686035\n",
      "get_velocity result - Requires grad: False\n",
      "Saved checkpoint to ../outputs/cubediff_mini/checkpoint-0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366de0b856064124aee5dd8cf087c9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime error: Error(s) in loading state_dict for CubeDiffModel:\n",
      "\tUnexpected key(s) in state_dict: \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\". \n",
      "Training error: Error(s) in loading state_dict for CubeDiffModel:\n",
      "\tUnexpected key(s) in state_dict: \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\". \n",
      "\n",
      "Error during training: Error(s) in loading state_dict for CubeDiffModel:\n",
      "\tUnexpected key(s) in state_dict: \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn1_lora.v_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_q.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_k.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_v.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.weight\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.base_attention.to_out.0.bias\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.q_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.k_lora.lora_B\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_A\", \"base_unet.mid_block.attentions.0.transformer_blocks.0.attn2_lora.v_lora.lora_B\". \n",
      "Saved checkpoint to ../outputs/cubediff_mini/error_checkpoint\n",
      "Saved checkpoint at training failure point\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/lr</td><td>▁</td></tr><tr><td>train/step</td><td>▁</td></tr><tr><td>train/time_minutes</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0</td></tr><tr><td>train/loss</td><td>2.19758</td></tr><tr><td>train/lr</td><td>0.0001</td></tr><tr><td>train/step</td><td>0</td></tr><tr><td>train/time_minutes</td><td>0.03608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync ../outputs/cubediff_mini/logs/wandb/wandb/offline-run-20250422_051414-9c92ugsj<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../outputs/cubediff_mini/logs/wandb/wandb/offline-run-20250422_051414-9c92ugsj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished wandb run\n",
      "Training loss curve:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try to run the training with improved error handling and optimal multiprocessing\n",
    "try:\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fix data types and device mismatch issues\n",
    "    trainer.fix_device_mismatch()\n",
    "    \n",
    "    # Enable progressive loading if configured\n",
    "    if hasattr(config, 'progressive_loading') and config.progressive_loading:\n",
    "        trainer.enable_progressive_loading(True)\n",
    "    \n",
    "    # Print optimization settings\n",
    "    print(f\"\\nTraining with optimized settings for GCP Vertex AI with L4 GPU:\")\n",
    "    print(f\"- Number of workers: {config.num_workers}\")\n",
    "    print(f\"- Batch size: {config.batch_size}\")\n",
    "    print(f\"- Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"- Mixed precision: {config.mixed_precision}\")\n",
    "    \n",
    "    if shm_size_gb is not None:\n",
    "        print(f\"- Available shared memory: {shm_size_gb:.2f} GB\")\n",
    "        if shm_size_gb < 8 and config.num_workers > 1:\n",
    "            print(\"⚠️ Warning: Shared memory is limited. If you encounter errors, consider:\")\n",
    "            print(\"  - Reducing num_workers further\")\n",
    "            print(\"  - Increasing shared memory with: sudo mount -o remount,size=16G /dev/shm\")\n",
    "    \n",
    "    # Apply adaptive settings based on shared memory\n",
    "    config = adaptive_shm_handling(config)\n",
    "\n",
    "    # First, fix the noise scheduler to handle FP16 inputs properly\n",
    "    trainer = patch_noise_scheduler(trainer)\n",
    "\n",
    "    # Enable progressive loading\n",
    "    if hasattr(config, 'progressive_loading') and config.progressive_loading:\n",
    "        trainer.enable_progressive_loading(True)\n",
    "    \n",
    "    # check model size\n",
    "    print_model_size(trainer.model)\n",
    "    \n",
    "    # Run training with optimized settings\n",
    "    print(f\"03 trainer.train() started ...\\n \")\n",
    "    trainer.train(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_train_epochs=num_steps\n",
    "    )\n",
    "    print(f\"03 trainer.train() done\\n \")\n",
    "    # Add memory monitoring to the trainer\n",
    "    trainer = add_memory_monitoring(trainer)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Average time per step: {training_time / num_steps:.2f} seconds\")\n",
    "    print(f\"Estimated time for 30000 steps: {(training_time / num_steps) * 30000 / 3600:.2f} hours\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError during training: {str(e)}\")\n",
    "    \n",
    "    # Check if this is a memory-related error\n",
    "    error_msg = str(e).lower()\n",
    "    if any(term in error_msg for term in [\"memory\", \"cuda\", \"shared memory\", \"bus error\", \"dataloader worker\"]):\n",
    "        print(\"\\nThis appears to be a memory-related error.\")\n",
    "        \n",
    "        # Suggest progressive reduction of workers\n",
    "        if config.num_workers > 0:\n",
    "            reduced_workers = max(0, config.num_workers - 1)\n",
    "            print(f\"\\nTry reducing num_workers from {config.num_workers} to {reduced_workers}\")\n",
    "            print(\"Add this code to your next cell:\")\n",
    "            print(f\"config.num_workers = {reduced_workers}\")\n",
    "            print(\"trainer = CubeDiffTrainer(config=config, ...)\")\n",
    "        else:\n",
    "            # If workers already at 0, suggest other optimizations\n",
    "            print(\"\\nSince num_workers is already 0, try these alternatives:\")\n",
    "            print(f\"1. Reduce batch_size (currently {config.batch_size})\")\n",
    "            print(f\"2. Increase gradient_accumulation_steps (currently {config.gradient_accumulation_steps})\")\n",
    "        \n",
    "        if \"shared memory\" in error_msg or \"bus error\" in error_msg:\n",
    "            print(\"\\nYou can also increase shared memory on your GCP instance:\")\n",
    "            print(\"1. Open a terminal in JupyterLab\")\n",
    "            print(\"2. Run: sudo mount -o remount,size=16G /dev/shm\")\n",
    "            print(\"3. Restart the kernel and try again\")\n",
    "    \n",
    "    # Save checkpoint if possible\n",
    "    try:\n",
    "        trainer.save_checkpoint(os.path.join(config.output_dir, \"error_checkpoint\"))\n",
    "        print(\"Saved checkpoint at training failure point\")\n",
    "    except Exception as save_err:\n",
    "        print(f\"Could not save checkpoint: {str(save_err)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUnexpected error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Clean up wandb\n",
    "    if trainer.wandb_run:\n",
    "        trainer.wandb_run.finish()\n",
    "        print(\"Finished wandb run\")\n",
    "    \n",
    "    # Show training visualizations if available\n",
    "    try:\n",
    "        # Alternative to wandb.jupyter.show() since it doesn't seem to work\n",
    "        loss_curve_path = os.path.join(trainer.logs_dir, 'loss_curve.png')\n",
    "        if os.path.exists(loss_curve_path):\n",
    "            from IPython.display import Image, display\n",
    "            print(\"Training loss curve:\")\n",
    "            display(Image(filename=loss_curve_path))\n",
    "        else:\n",
    "            print(\"No loss curve found.\")\n",
    "    except Exception as vis_err:\n",
    "        print(f\"Could not show visualizations: {str(vis_err)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress\n",
    "\n",
    "Since we've configured wandb in offline mode, we can visualize the training using the built-in visualization method from the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trainer's built-in visualization method\n",
    "try:\n",
    "    trainer.show_training_visualizations()\n",
    "except Exception as e:\n",
    "    print(f\"Could not show visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Offline wandb Logs\n",
    "\n",
    "We can also view the stored wandb logs directly in JupyterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View wandb logs directly using wandb.jupyter.show()\n",
    "# if trainer.wandb_run:\n",
    "#     wandb.jupyter.show()\n",
    "\n",
    "# Properly handle wandb offline mode visualization\n",
    "# View logs locally since wandb.jupyter.show() doesn't work\n",
    "# All logs will be kept on the GCP instance\n",
    "\n",
    "# Properly finish wandb run if it's still active\n",
    "if trainer.wandb_run and hasattr(trainer.wandb_run, 'id'):\n",
    "    trainer.wandb_run.finish()\n",
    "    print(\"Finished wandb run\")\n",
    "    \n",
    "    # Print offline sync information\n",
    "    run_id = trainer.wandb_run.id\n",
    "    run_path = os.path.join(trainer.logs_dir, \"wandb\", \"offline-run-\" + run_id)\n",
    "    if os.path.exists(run_path):\n",
    "        print(f\"Wandb offline run saved at: {run_path}\")\n",
    "        print(f\"To sync later, you can run: wandb sync {run_path}\")\n",
    "\n",
    "# Display loss curve directly from saved files\n",
    "loss_curve_path = os.path.join(trainer.logs_dir, 'loss_curve.png')\n",
    "if os.path.exists(loss_curve_path):\n",
    "    from IPython.display import Image, display\n",
    "    print(\"Training loss curve:\")\n",
    "    display(Image(filename=loss_curve_path))\n",
    "else:\n",
    "    # Try to find loss data in CSV and plot it\n",
    "    log_file = os.path.join(trainer.logs_dir, \"training_log.csv\")\n",
    "    if os.path.exists(log_file):\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        print(\"Plotting loss from training log file...\")\n",
    "        logs = pd.read_csv(log_file)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(logs['step'], logs['loss'])\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(trainer.logs_dir, 'generated_loss_curve.png'))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No loss curve or training log found.\")\n",
    "\n",
    "# Display samples if available\n",
    "samples_dir = os.path.join(trainer.images_dir)\n",
    "if os.path.exists(samples_dir):\n",
    "    sample_dirs = [d for d in os.listdir(samples_dir) if d.startswith('step_')]\n",
    "    if sample_dirs:\n",
    "        # Get the latest sample directory\n",
    "        latest_dir = sorted(sample_dirs, key=lambda x: int(x.split('_')[1]))[-1]\n",
    "        latest_samples_dir = os.path.join(samples_dir, latest_dir)\n",
    "        \n",
    "        # Display samples\n",
    "        sample_files = [f for f in os.listdir(latest_samples_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        if sample_files:\n",
    "            from IPython.display import Image, display\n",
    "            \n",
    "            print(f\"\\nDisplaying latest samples from {latest_dir}:\")\n",
    "            for sample_file in sample_files[:3]:  # Show up to 3 samples\n",
    "                print(f\"Sample: {sample_file}\")\n",
    "                display(Image(os.path.join(latest_samples_dir, sample_file)))\n",
    "        else:\n",
    "            print(\"No sample images found in the directory.\")\n",
    "    else:\n",
    "        print(\"No sample directories found.\")\n",
    "\n",
    "# Display local wandb files status\n",
    "try:\n",
    "    wandb_dir = os.path.join(trainer.logs_dir, \"wandb\")\n",
    "    if os.path.exists(wandb_dir):\n",
    "        print(\"\\nWandb offline files:\")\n",
    "        runs = [d for d in os.listdir(wandb_dir) if d.startswith('offline-run-')]\n",
    "        \n",
    "        if runs:\n",
    "            for run in runs:\n",
    "                run_path = os.path.join(wandb_dir, run)\n",
    "                run_size = sum(os.path.getsize(os.path.join(dirpath, filename)) \n",
    "                            for dirpath, _, filenames in os.walk(run_path) \n",
    "                            for filename in filenames) / (1024 * 1024)  # Size in MB\n",
    "                \n",
    "                print(f\"- {run} ({run_size:.1f} MB)\")\n",
    "                \n",
    "                # Look for log files\n",
    "                log_dir = os.path.join(run_path, \"logs\")\n",
    "                if os.path.exists(log_dir):\n",
    "                    log_files = os.listdir(log_dir)\n",
    "                    if log_files:\n",
    "                        print(f\"  Log files: {', '.join(log_files[:3])}\" + (\"...\" if len(log_files) > 3 else \"\"))\n",
    "        else:\n",
    "            print(\"No offline runs found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking wandb files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alternative: View Loss Curve Directly\n",
    "\n",
    "If for some reason wandb visualization is not working as expected, we can view the loss curve directly from the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loss curve from the logs directory\n",
    "logs_dir = os.path.join(config.output_dir, \"logs\")\n",
    "loss_curve_path = os.path.join(logs_dir, 'loss_curve.png')\n",
    "\n",
    "if os.path.exists(loss_curve_path):\n",
    "    from IPython.display import Image\n",
    "    display(Image(filename=loss_curve_path))\n",
    "else:\n",
    "    print(\"Loss curve image not found.\")\n",
    "    \n",
    "    # Check if we have training logs that we can plot\n",
    "    log_file = os.path.join(logs_dir, \"training_log.csv\")\n",
    "    if os.path.exists(log_file):\n",
    "        import pandas as pd\n",
    "        logs = pd.read_csv(log_file)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(logs['step'], logs['loss'])\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Generated Samples\n",
    "\n",
    "Let's check if any samples were generated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for sample images\n",
    "# View any generated samples\n",
    "images_dir = os.path.join(config.output_dir, \"samples\")\n",
    "if os.path.exists(images_dir):\n",
    "    sample_dirs = [d for d in os.listdir(images_dir) if d.startswith('step_')]\n",
    "    if sample_dirs:\n",
    "        # Get the latest sample directory\n",
    "        latest_dir = sorted(sample_dirs, key=lambda x: int(x.split('_')[1]))[-1]\n",
    "        latest_samples_dir = os.path.join(images_dir, latest_dir)\n",
    "        \n",
    "        # Display samples\n",
    "        sample_files = [f for f in os.listdir(latest_samples_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        if sample_files:\n",
    "            from IPython.display import Image, display\n",
    "            \n",
    "            print(f\"Displaying samples from {latest_dir}:\\n\")\n",
    "            for sample_file in sample_files:\n",
    "                print(f\"Sample: {sample_file}\")\n",
    "                display(Image(os.path.join(latest_samples_dir, sample_file)))\n",
    "                print(\"\\n\")\n",
    "        else:\n",
    "            print(\"No sample images found in the directory.\")\n",
    "    else:\n",
    "        print(\"No sample directories found.\")\n",
    "else:\n",
    "    print(\"Samples directory not found.\")\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Mini-training completed: {num_steps} steps\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Workers: {config.num_workers}\")\n",
    "print(f\"Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Checkpoint Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest checkpoint\n",
    "checkpoints = [d for d in os.listdir(config.output_dir) if d.startswith('checkpoint-')]\n",
    "if checkpoints:\n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "    latest_checkpoint = os.path.join(config.output_dir, checkpoints[-1], \"model.pt\")\n",
    "    \n",
    "    print(f\"Latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Load the checkpoint to test it\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        try:\n",
    "            # Initialize a new model\n",
    "            test_model = CubeDiffModel(config.pretrained_model_name)\n",
    "            \n",
    "            # Load the checkpoint\n",
    "            test_model.load_state_dict(torch.load(latest_checkpoint, map_location=\"cpu\"))\n",
    "            \n",
    "            print(\"Successfully loaded checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    else:\n",
    "        print(\"Checkpoint file not found\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Estimate Full Training Resources\n",
    "\n",
    "Based on the mini-training, let's estimate the resources needed for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate full training resources\n",
    "if 'training_time' in locals():\n",
    "    # Estimated total training time\n",
    "    total_hours = (training_time / num_steps) * 30000 / 3600\n",
    "    \n",
    "    # With 8-hour daily sessions\n",
    "    days_needed = total_hours / 8\n",
    "    \n",
    "    print(f\"Estimated resources for full 30,000 steps training:\")\n",
    "    print(f\"Total training time: {total_hours:.2f} hours\")\n",
    "    print(f\"Training days with 8-hour sessions: {days_needed:.2f} days\")\n",
    "    \n",
    "    # Estimate GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        # Get current GPU memory usage\n",
    "        memory_used = torch.cuda.max_memory_allocated(0) / 1024 / 1024 / 1024  # GB\n",
    "        print(f\"Peak GPU memory usage: {memory_used:.2f} GB\")\n",
    "        \n",
    "        # Check if we need to optimize further\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024  # GB\n",
    "        if memory_used > 0.9 * total_memory:\n",
    "            print(\"WARNING: Memory usage is close to capacity. Consider reducing batch size or model size.\")\n",
    "        else:\n",
    "            print(f\"Memory headroom: {(total_memory - memory_used):.2f} GB\")\n",
    "    \n",
    "    # Estimate cost (assuming $1.25/hour for L4 GPU on GCP)\n",
    "    hourly_rate = 1.25 * torch.cuda.device_count()  # Cost per hour for all GPUs\n",
    "    estimated_cost = hourly_rate * total_hours\n",
    "    \n",
    "    print(f\"Estimated cost (at ${hourly_rate:.2f}/hour): ${estimated_cost:.2f}\")\n",
    "    \n",
    "    # Check if within budget\n",
    "    daily_budget = 80  # Daily budget in dollars\n",
    "    daily_cost = hourly_rate * 8  # Cost for 8 hours of training\n",
    "    \n",
    "    print(f\"Daily cost (8 hours): ${daily_cost:.2f} (Budget: ${daily_budget})\") \n",
    "    if daily_cost > daily_budget:\n",
    "        print(\"WARNING: Daily cost exceeds budget.\")\n",
    "else:\n",
    "    print(\"No training data available for estimation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup wandb Run\n",
    "\n",
    "Make sure to finish the wandb run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly finish wandb run if it's still active\n",
    "if trainer.wandb_run and trainer.wandb_run.id:\n",
    "    trainer.wandb_run.finish()\n",
    "    print(\"Finished wandb run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "Based on this mini-training session, here are the next steps for full CubeDiff implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Expand the dataset**:\n",
    "   - Collect and process more panorama images\n",
    "   - Ensure diversity in scene types (indoor/outdoor, natural/urban)\n",
    "   - Create high-quality captions for all images\n",
    "\n",
    "2. **Optimize training parameters**:\n",
    "   - Adjust learning rate and scheduling based on mini-training results\n",
    "   - Fine-tune LoRA parameters for better efficiency\n",
    "   - Consider gradient accumulation steps for larger effective batch size\n",
    "\n",
    "3. **Set up long-running training**:\n",
    "   - Configure automatic checkpointing for 8-hour sessions\n",
    "   - Implement a robust session management system\n",
    "   - Ensure training can be resumed from checkpoints\n",
    "\n",
    "4. **Implement evaluation metrics**:\n",
    "   - Face consistency measures\n",
    "   - FID score for panorama quality\n",
    "   - Text-image alignment metrics for conditional generation\n",
    "\n",
    "5. **Create inference pipeline**:\n",
    "   - Build an optimized inference system\n",
    "   - Implement text-to-panorama and image-to-panorama modes\n",
    "   - Optimize for high-resolution output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
