{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubeDiff: Training Pipeline\n",
    "\n",
    "This notebook demonstrates the training pipeline for CubeDiff:\n",
    "\n",
    "1. Configure training parameters\n",
    "2. Initialize the model and training components\n",
    "3. Train on a small dataset\n",
    "4. Monitor training progress\n",
    "5. Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add parent directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import custom modules\n",
    "from model.architecture import CubeDiffModel\n",
    "from data.dataset import CubemapDataset, get_dataloader\n",
    "from training.trainer import CubeDiffTrainer\n",
    "from training.lora import add_lora_to_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration class\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model config\n",
    "        self.pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.lora_rank = 16\n",
    "        self.lora_alpha = 16\n",
    "        self.prediction_type = \"v_prediction\"  # or \"epsilon\"\n",
    "        \n",
    "        # Training config\n",
    "        self.output_dir = \"../outputs/cubediff_mini\"\n",
    "        self.data_dir = \"../data/processed/cubemaps\"\n",
    "        self.captions_file = \"../data/processed/captions.json\"\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 1e-4\n",
    "        self.min_learning_rate = 1e-6\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_workers = 2\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.mixed_precision = \"fp16\"\n",
    "        \n",
    "        # Logging config\n",
    "        self.use_wandb = False\n",
    "        self.wandb_project = \"cubediff\"\n",
    "        self.wandb_run_name = \"cubediff_mini\"\n",
    "        self.log_every_n_steps = 10\n",
    "        self.save_every_n_steps = 100\n",
    "        self.eval_every_n_steps = 100\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check\n",
    "\n",
    "Let's confirm that we have the required resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024:.2f} GB\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"GPU not available, training will be slow on CPU.\")\n",
    "\n",
    "# Check directory structure\n",
    "data_path_exists = os.path.exists(config.data_dir)\n",
    "print(f\"Data directory exists: {data_path_exists}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CubemapDataset(\n",
    "    data_dir=config.data_dir,\n",
    "    captions_file=config.captions_file\n",
    ")\n",
    "\n",
    "# For this mini example, we'll use the same dataset for validation\n",
    "val_dataset = train_dataset\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample caption: {sample['caption']}\")\n",
    "print(f\"Sample faces shape: {sample['faces'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = CubeDiffTrainer(\n",
    "    config=config,\n",
    "    pretrained_model_name=config.pretrained_model_name,\n",
    "    output_dir=config.output_dir,\n",
    "    mixed_precision=config.mixed_precision,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Mini-Training\n",
    "\n",
    "For this demonstration, we'll run a small number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a mini training session\n",
    "num_steps = 250  # Just for demonstration\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_train_epochs=num_steps\n",
    ")\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Average time per step: {training_time / num_steps:.2f} seconds\")\n",
    "print(f\"Estimated time for 30000 steps: {(training_time / num_steps) * 30000 / 3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If wandb was used, we can visualize the training curves\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    \n",
    "    # Get run history\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{config.wandb_project}/{config.wandb_run_name}\")\n",
    "    \n",
    "    # Get loss history\n",
    "    history = run.history()\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train/loss'], label='Training Loss')\n",
    "    if 'val/loss' in history.columns:\n",
    "        plt.plot(history['val/loss'], label='Validation Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"WandB logging was disabled, no training curves available.\")\n",
    "    \n",
    "    # Instead, let's look at the checkpoint files\n",
    "    checkpoint_dir = config.output_dir\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "        print(f\"Saved checkpoints: {checkpoints}\")\n",
    "        \n",
    "        # Plot checkpoint sizes to get a rough idea of model evolution\n",
    "        sizes = []\n",
    "        steps = []\n",
    "        \n",
    "        for ckpt in checkpoints:\n",
    "            ckpt_path = os.path.join(checkpoint_dir, ckpt, \"model.pt\")\n",
    "            if os.path.exists(ckpt_path):\n",
    "                size = os.path.getsize(ckpt_path) / (1024 * 1024)  # MB\n",
    "                step = int(ckpt.split('-')[1])\n",
    "                sizes.append(size)\n",
    "                steps.append(step)\n",
    "        \n",
    "        if sizes:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(steps, sizes, 'o-')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Checkpoint Size (MB)')\n",
    "            plt.title('Model Checkpoint Size Evolution')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No checkpoint files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Checkpoint Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest checkpoint\n",
    "checkpoints = [d for d in os.listdir(config.output_dir) if d.startswith('checkpoint-')]\n",
    "if checkpoints:\n",
    "    # Sort by step number\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "    latest_checkpoint = os.path.join(config.output_dir, checkpoints[-1], \"model.pt\")\n",
    "    \n",
    "    print(f\"Latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Load the checkpoint to test it\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        try:\n",
    "            # Initialize a new model\n",
    "            test_model = CubeDiffModel(config.pretrained_model_name)\n",
    "            \n",
    "            # Load the checkpoint\n",
    "            test_model.load_state_dict(torch.load(latest_checkpoint, map_location=\"cpu\"))\n",
    "            \n",
    "            print(\"Successfully loaded checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    else:\n",
    "        print(\"Checkpoint file not found\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Estimate Full Training Resources\n",
    "\n",
    "Based on the mini-training, let's estimate the resources needed for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate full training resources\n",
    "if 'training_time' in locals():\n",
    "    # Estimated total training time\n",
    "    total_hours = (training_time / num_steps) * 30000 / 3600\n",
    "    \n",
    "    # With 8-hour daily sessions\n",
    "    days_needed = total_hours / 8\n",
    "    \n",
    "    print(f\"Estimated resources for full 30,000 steps training:\")\n",
    "    print(f\"Total training time: {total_hours:.2f} hours\")\n",
    "    print(f\"Training days with 8-hour sessions: {days_needed:.2f} days\")\n",
    "    \n",
    "    # Estimate GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        # Get current GPU memory usage\n",
    "        memory_used = torch.cuda.max_memory_allocated(0) / 1024 / 1024 / 1024  # GB\n",
    "        print(f\"Peak GPU memory usage: {memory_used:.2f} GB\")\n",
    "        \n",
    "        # Check if we need to optimize further\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024  # GB\n",
    "        if memory_used > 0.9 * total_memory:\n",
    "            print(\"WARNING: Memory usage is close to capacity. Consider reducing batch size or model size.\")\n",
    "        else:\n",
    "            print(f\"Memory headroom: {(total_memory - memory_used):.2f} GB\")\n",
    "    \n",
    "    # Estimate cost (assuming $1.25/hour for L4 GPU on GCP)\n",
    "    hourly_rate = 1.25 * torch.cuda.device_count()  # Cost per hour for all GPUs\n",
    "    estimated_cost = hourly_rate * total_hours\n",
    "    \n",
    "    print(f\"Estimated cost (at ${hourly_rate:.2f}/hour): ${estimated_cost:.2f}\")\n",
    "    \n",
    "    # Check if within budget\n",
    "    daily_budget = 80  # Daily budget in dollars\n",
    "    daily_cost = hourly_rate * 8  # Cost for 8 hours of training\n",
    "    \n",
    "    print(f\"Daily cost (8 hours): ${daily_cost:.2f} (Budget: ${daily_budget})\") \n",
    "    if daily_cost > daily_budget:\n",
    "        print(\"WARNING: Daily cost exceeds budget.\")\n",
    "else:\n",
    "    print(\"No training data available for estimation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Based on this mini-training session, here are the next steps for full CubeDiff implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Expand the dataset**:\n",
    "   - Collect and process more panorama images\n",
    "   - Ensure diversity in scene types (indoor/outdoor, natural/urban)\n",
    "   - Create high-quality captions for all images\n",
    "\n",
    "2. **Optimize training parameters**:\n",
    "   - Adjust learning rate and scheduling based on mini-training results\n",
    "   - Fine-tune LoRA parameters for better efficiency\n",
    "   - Consider gradient accumulation steps for larger effective batch size\n",
    "\n",
    "3. **Set up long-running training**:\n",
    "   - Configure automatic checkpointing for 8-hour sessions\n",
    "   - Implement a robust session management system\n",
    "   - Ensure training can be resumed from checkpoints\n",
    "\n",
    "4. **Implement evaluation metrics**:\n",
    "   - Face consistency measures\n",
    "   - FID score for panorama quality\n",
    "   - Text-image alignment metrics for conditional generation\n",
    "\n",
    "5. **Create inference pipeline**:\n",
    "   - Build an optimized inference system\n",
    "   - Implement text-to-panorama and image-to-panorama modes\n",
    "   - Optimize for high-resolution output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}